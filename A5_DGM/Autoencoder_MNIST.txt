#q1 extra
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from collections import Counter

# -------------------------------
# Step 1: Load MNIST dataset
# -------------------------------
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(-1))  # flatten 28x28 -> 784
])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)

# -------------------------------
# Step 2: Explore Dataset
# -------------------------------
print(f"Number of training samples: {len(train_dataset)}")
print(f"Number of test samples: {len(test_dataset)}")

# Sample images
examples = iter(train_loader)
example_data, example_targets = next(examples)

plt.figure(figsize=(12,6))
for i in range(12):
    plt.subplot(3,4,i+1)
    plt.imshow(example_data[i].view(28,28), cmap='gray')
    plt.title(f"Label: {example_targets[i]}")
    plt.axis('off')
plt.suptitle("Sample MNIST Images")
plt.show()

# Distribution of digits in training set
train_labels = [label for _, label in train_dataset]
label_counts = Counter(train_labels)
plt.figure(figsize=(6,4))
plt.bar(label_counts.keys(), label_counts.values())
plt.xlabel("Digit")
plt.ylabel("Frequency")
plt.title("Digit Distribution in Training Set")
plt.show()

# -------------------------------
# Step 3: Define Autoencoder
# -------------------------------
class Autoencoder(nn.Module):
    def __init__(self, input_dim=784, encoded_dim=64):
        super(Autoencoder, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, encoded_dim),
            nn.ReLU()
        )
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(encoded_dim, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded

autoencoder = Autoencoder()
criterion_ae = nn.MSELoss()
optimizer_ae = optim.Adam(autoencoder.parameters(), lr=0.001)

# -------------------------------
# Step 4: Train Autoencoder
# -------------------------------
n_epochs = 10
ae_losses = []

for epoch in range(n_epochs):
    epoch_loss = 0
    for batch, _ in train_loader:
        encoded, decoded = autoencoder(batch)
        loss = criterion_ae(decoded, batch)
        optimizer_ae.zero_grad()
        loss.backward()
        optimizer_ae.step()
        epoch_loss += loss.item()
    ae_losses.append(epoch_loss/len(train_loader))
    print(f"Epoch {epoch+1}/{n_epochs}, Autoencoder Loss: {ae_losses[-1]:.4f}")

# Plot Autoencoder Training Loss
plt.figure(figsize=(6,4))
plt.plot(ae_losses, marker='o')
plt.title("Autoencoder Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

# -------------------------------
# Step 5: Visualize Reconstruction
# -------------------------------
with torch.no_grad():
    _, decoded_data = autoencoder(example_data)

plt.figure(figsize=(12,4))
for i in range(10):
    # Original
    plt.subplot(2,10,i+1)
    plt.imshow(example_data[i].view(28,28), cmap='gray')
    plt.axis('off')
    if i == 0: plt.title("Original")
    # Reconstructed
    plt.subplot(2,10,10+i+1)
    plt.imshow(decoded_data[i].view(28,28), cmap='gray')
    plt.axis('off')
    if i == 0: plt.title("Reconstructed")
plt.suptitle("Autoencoder Reconstruction")
plt.show()

# -------------------------------
# Step 6: Use encoded features for classification
# -------------------------------
class Classifier(nn.Module):
    def __init__(self, encoded_dim=64, n_classes=10):
        super(Classifier, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(encoded_dim, 64),
            nn.ReLU(),
            nn.Linear(64, n_classes)
        )
    def forward(self, x):
        return self.fc(x)

classifier = Classifier(encoded_dim=64)
criterion_clf = nn.CrossEntropyLoss()
optimizer_clf = optim.Adam(classifier.parameters(), lr=0.001)

# Prepare encoded features
def get_encoded_features(autoencoder, loader):
    features = []
    labels = []
    with torch.no_grad():
        for batch, batch_labels in loader:
            encoded, _ = autoencoder(batch)
            features.append(encoded)
            labels.append(batch_labels)
    return torch.vstack(features), torch.hstack(labels)

train_features, train_labels = get_encoded_features(autoencoder, train_loader)
test_features, test_labels = get_encoded_features(autoencoder, test_loader)

train_dataset_encoded = torch.utils.data.TensorDataset(train_features, train_labels)
test_dataset_encoded = torch.utils.data.TensorDataset(test_features, test_labels)

train_loader_encoded = torch.utils.data.DataLoader(train_dataset_encoded, batch_size=128, shuffle=True)
test_loader_encoded = torch.utils.data.DataLoader(test_dataset_encoded, batch_size=128, shuffle=False)

# -------------------------------
# Step 7: Train Classifier
# -------------------------------
n_epochs_clf = 10
clf_losses = []

for epoch in range(n_epochs_clf):
    epoch_loss = 0
    for batch, labels in train_loader_encoded:
        outputs = classifier(batch)
        loss = criterion_clf(outputs, labels)
        optimizer_clf.zero_grad()
        loss.backward()
        optimizer_clf.step()
        epoch_loss += loss.item()
    clf_losses.append(epoch_loss/len(train_loader_encoded))
    print(f"Epoch {epoch+1}/{n_epochs_clf}, Classifier Loss: {clf_losses[-1]:.4f}")

# Plot Classifier Training Loss
plt.figure(figsize=(6,4))
plt.plot(clf_losses, marker='o', color='orange')
plt.title("Classifier Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

# -------------------------------
# Step 8: Evaluate Classifier
# -------------------------------
classifier.eval()
correct = 0
total = 0
with torch.no_grad():
    for batch, labels in test_loader_encoded:
        outputs = classifier(batch)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f"Test Accuracy: {100*correct/total:.2f}%")
