#q2
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Explore the dataset
print("🔍 Training Data Shape:", x_train.shape)
print("🔍 Testing Data Shape:", x_test.shape)
print("🔍 Sample Label:", y_train[0])

# Visualize a few samples
plt.figure(figsize=(8, 4))
for i in range(6):
    plt.subplot(2, 3, i + 1)
    plt.imshow(x_train[i], cmap='gray')
    plt.title(f"Label: {y_train[i]}")
    plt.axis('off')
plt.tight_layout()
plt.show()

# Normalize and reshape the data
x_train = x_train / 255.0
x_test = x_test / 255.0
x_train = x_train.reshape(-1, 784)  # 28x28 -> 784
x_test = x_test.reshape(-1, 784)

# One-hot encode labels
y_train_cat = to_categorical(y_train, 10)
y_test_cat = to_categorical(y_test, 10)

# Explore the pixel values and label distribution
df = pd.DataFrame(x_train[:5000])  # Use subset to save memory
df['label'] = y_train[:5000]

print("\n Sample Data (flattened):")
print(df.head())

print("\n DataFrame Info:")
print(df.info())

print("\n Pixel Value Statistics:")
print(df.drop(columns='label').describe().T)

unique, counts = np.unique(y_train, return_counts=True)
label_dist = dict(zip(unique, counts))

print("\nFull Training Label Distribution:")
for label, count in label_dist.items():
    print(f"Digit {label}: {count} samples")

# Class distribution plot
plt.figure(figsize=(8, 4))
sns.barplot(x=list(label_dist.keys()), y=list(label_dist.values()))
plt.title("Digit Frequency in Full Training Set")
plt.xlabel("Digit Class")
plt.ylabel("Count")
plt.show()

# Pixel intensity range
print("\n Pixel intensity range in training data:")
print(f"Min: {x_train.min()}, Max: {x_train.max()}")

# Visualize average image per digit
average_images = np.zeros((10, 28, 28))
for i in range(10):
    average_images[i] = np.mean(x_train[y_train == i].reshape(-1, 28, 28), axis=0)

plt.figure(figsize=(12, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(average_images[i], cmap='hot')
    plt.title(f"Avg Digit {i}")
    plt.axis('off')
plt.suptitle("Average Image per Digit Class", fontsize=16)
plt.tight_layout()
plt.show()

# Build the MLP model
model = Sequential([
    Dense(160, activation='relu', input_shape=(784,)),
    Dense(480, activation='relu'),
    Dense(256, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile the model
learning_rate = 0.01
optimizer = Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Print model summary
print("\n Model Architecture:")
model.summary()

# Train the model
print("\n Training the Model (Epochs = 10, Batch Size = 64, Validation Split = 0.2):")
history = model.fit(
    x_train, y_train_cat,
    validation_split=0.2,
    epochs=10,
    batch_size=64,
    verbose=2
)

# Evaluate on test data
test_loss, test_accuracy = model.evaluate(x_test, y_test_cat, verbose=0)
print(f"\n Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"Test Loss: {test_loss:.4f}")

# Plot training & validation accuracy/loss
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy Over Epochs')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title(' Loss Over Epochs')
plt.legend()

plt.tight_layout()
plt.show()