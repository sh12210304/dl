#q1.)
import numpy as np

# Sigmoid activation and derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# XOR truth table
X = np.array([[0,0],
              [0,1],
              [1,0],
              [1,1]])
y = np.array([[0],
              [1],
              [1],
              [0]])

# Network parameters
input_neurons = 2
hidden1_neurons = 2
hidden2_neurons = 2
output_neurons = 1
learning_rate = 1.5
epochs = 5000
tolerance = 0.01

# Initialize W1 with given values
W1 = np.array([[1, -0.5],   # w11=1, w12=-0.5
               [0.5, 1]])   # w21=0.5, w22=1

# Initialize other weights randomly
np.random.seed(42)
W2 = np.random.uniform(-1, 1, (hidden1_neurons, hidden2_neurons))
W3 = np.random.uniform(-1, 1, (hidden2_neurons, output_neurons))

# Biases
b1 = np.zeros((1, hidden1_neurons))
b2 = np.zeros((1, hidden2_neurons))
b3 = np.zeros((1, output_neurons))

print("Initial Weights and Biases")
print("W1:\n", W1)
print("W2:\n", W2)
print("W3:\n", W3, "\n")

# Training
for epoch in range(1, epochs+1):
    for i in range(len(X)):
        # Forward pass
        z1 = np.dot(X[i].reshape(1,-1), W1) + b1
        a1 = sigmoid(z1)

        z2 = np.dot(a1, W2) + b2
        a2 = sigmoid(z2)

        z3 = np.dot(a2, W3) + b3
        output = sigmoid(z3)

        # Error
        error = y[i] - output

        # Backpropagation only if error is large
        if abs(error[0][0]) > tolerance:
            d_output = error * sigmoid_derivative(output)
            d_hidden2 = d_output.dot(W3.T) * sigmoid_derivative(a2)
            d_hidden1 = d_hidden2.dot(W2.T) * sigmoid_derivative(a1)

            # Weight updates
            W3_update = a2.T.dot(d_output) * learning_rate
            W2_update = a1.T.dot(d_hidden2) * learning_rate
            W1_update = X[i].reshape(1,-1).T.dot(d_hidden1) * learning_rate

            b3_update = d_output * learning_rate
            b2_update = d_hidden2 * learning_rate
            b1_update = d_hidden1 * learning_rate

            # Apply updates
            W3 += W3_update
            W2 += W2_update
            W1 += W1_update
            b3 += b3_update
            b2 += b2_update
            b1 += b1_update

            # Show per-pattern updates only for the first epoch
            if epoch == 1:
                print(f"Epoch {epoch}, Input: {X[i]}, Expected: {y[i][0]}, Predicted: {output[0][0]:.4f}, Error: {error[0][0]:.4f}")
                print("Weights updated ")
                print("Updated W1:\n", W1)
                print("Updated W2:\n", W2)
                print("Updated W3:\n", W3)
                print("Updated b1:", b1)
                print("Updated b2:", b2)
                print("Updated b3:", b3, "\n")
        else:
            if epoch == 1:
                print(f"Epoch {epoch}, Input: {X[i]}, Expected: {y[i][0]}, Predicted: {output[0][0]:.4f}, Error: {error[0][0]:.4f}")
                print("No weight update required ❌\n")

    # Show summary every 1000 epochs
    if epoch % 1000 == 0:
        loss = np.mean(np.square(y - output))
        print(f"Epoch {epoch}, Loss={loss:.4f}")
        print("W1:\n", W1)
        print("W2:\n", W2)
        print("W3:\n", W3, "\n")

# Final predictions after training
print("\nFinal Predictions after training:")
for i in range(len(X)):
    z1 = np.dot(X[i].reshape(1,-1), W1) + b1
    a1 = sigmoid(z1)
    z2 = np.dot(a1, W2) + b2
    a2 = sigmoid(z2)
    z3 = np.dot(a2, W3) + b3
    output = sigmoid(z3)
    print(f"Input: {X[i]} -> Predicted: {output[0][0]:.4f}, Expected: {y[i][0]}")
