#q3 with sgd
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.initializers import RandomNormal

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Explore dataset
print("Training data shape:", x_train.shape)
print("Training labels shape:", y_train.shape)
print("Test data shape:", x_test.shape)
print("Test labels shape:", y_test.shape)

# Show sample images
plt.figure(figsize=(10,4))
for i in range(10):
    plt.subplot(2,5,i+1)
    plt.imshow(x_train[i], cmap='gray')
    plt.title(f"Label: {y_train[i]}")
    plt.axis('off')
plt.suptitle("Sample MNIST Digits")
plt.show()

# Check class distribution
unique, counts = np.unique(y_train, return_counts=True)
print("Class distribution in training set:")
for label, count in zip(unique, counts):
    print(f"Digit {label}: {count} samples")

# Preprocess data
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Parameters
learning_rate = 0.01
batch_size = 100
num_iters = 3000  # Number of iterations you want to run

num_samples = x_train.shape[0]

# Calculate number of epochs from iterations:
epochs = (num_iters * batch_size) / num_samples
epochs = int(np.ceil(epochs))  # round up to next integer

print(f"Number of samples: {num_samples}")
print(f"Batch size: {batch_size}")
print(f"Number of iterations: {num_iters}")
print(f"Calculated number of epochs: {epochs}")

print("\nEpochs formula:")
print("epochs = (num_iters * batch_size) / num_samples\n")

initializer = RandomNormal(mean=0.0, stddev=0.05)

# Build the model
model = Sequential([
    Flatten(input_shape=(28,28)),
    Dense(160, activation='relu', kernel_initializer=initializer),
    Dense(480, activation='relu', kernel_initializer=initializer),
    Dense(256, activation='relu', kernel_initializer=initializer),
    Dense(10, activation='softmax', kernel_initializer=initializer)
])

optimizer = SGD(learning_rate=learning_rate)

model.compile(optimizer=optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Calculate iterations per epoch for reference
iterations_per_epoch = num_samples // batch_size
print(f'Iterations per epoch: {iterations_per_epoch}\n')

# Train the model for calculated epochs
history = model.fit(x_train, y_train,
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_split=0.2)

# Evaluate model
loss, accuracy = model.evaluate(x_test, y_test)
print(f'Test accuracy: {accuracy:.4f}')

# Plot accuracy and loss
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy over epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss over epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()